{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "227e2ae8-0f40-407f-b4e0-e89417450a26",
   "metadata": {},
   "source": [
    "## Byte Pair Encoder (BPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d64cf437-364b-4a88-b726-36137c072f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.8.0\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import tiktoken\n",
    "\n",
    "print(\"tiktoken version:\", importlib.metadata.version('tiktoken'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0e344e1-4b5a-4bb6-b640-e8196e7869f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3986b89b-9f6d-442e-b287-7ca90fb1eed2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15496,\n",
       " 11,\n",
       " 466,\n",
       " 345,\n",
       " 588,\n",
       " 8887,\n",
       " 30,\n",
       " 220,\n",
       " 50256,\n",
       " 554,\n",
       " 262,\n",
       " 4252,\n",
       " 18250,\n",
       " 8812,\n",
       " 2114,\n",
       " 286,\n",
       " 617,\n",
       " 34680,\n",
       " 27271]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = (\"Hello, do you like tea? <|endoftext|> In the sunlit terraces of someunknownPlace\")\n",
    "integers = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
    "integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f0cf5b6-af75-4426-b697-dfeb5385eeb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' you'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([345])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "94edf2eb-128d-42d3-8958-d2be1ecddbba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[33901, 86, 343, 86, 220, 959]\n",
      "Akwirw ier\n"
     ]
    }
   ],
   "source": [
    "integers = tokenizer.encode(\"Akwirw ier\")\n",
    "print(integers)\n",
    "\n",
    "strings = tokenizer.decode(integers)\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf42c2d6-224f-44a6-8d05-fd907c95e9e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ier'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([959])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "958adba4-bd98-4afa-aa77-2d211028f038",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The vocabulary size for GPT2 is: 50,257\n",
      "The vocabulary size for GPT3 is: 50,281\n",
      "The vocabulary size for GPT4 is: 100,277\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "# Initialize the encodings for GPT-2, GPT-3, and GPT-4\n",
    "encodings = {\n",
    "    \"gpt2\": tiktoken.get_encoding(\"gpt2\"),\n",
    "    \"gpt3\": tiktoken.get_encoding(\"p50k_base\"), # Commonly associated with GPT-3 models\n",
    "    \"gpt4\": tiktoken.get_encoding(\"cl100k_base\") # Used for GPT-4 and later versions\n",
    "}\n",
    "\n",
    "# Get the vocabulary size for each encoding\n",
    "vocab_sizes = {model: encoding.n_vocab for model, encoding in encodings.items()}\n",
    "\n",
    "# Print the vocabulary sizes\n",
    "for model, size in vocab_sizes.items():\n",
    "    print(f\"The vocabulary size for {model.upper()} is: {size:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d42f7c2-8caa-47ee-8b8e-24344b72adc2",
   "metadata": {},
   "source": [
    "## Creating Input-target Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ca64c1d4-8d21-49f9-aca3-d8cab8f1effa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5145\n"
     ]
    }
   ],
   "source": [
    "# Open the file in read mode\n",
    "with open('verdict.txt', 'r', encoding='utf-8') as file:\n",
    "    # Read the contents of the file\n",
    "    raw_text = file.read()\n",
    "\n",
    "enc_text = tokenizer.encode(raw_text)\n",
    "print(len(enc_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7945fca-e669-426a-9091-4c11351cc632",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_sample = enc_text[50:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ce0d729f-dcc1-424d-b55b-bab639894eb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: [290, 4920, 2241, 287]\n",
      "y: [4920, 2241, 287, 257]\n"
     ]
    }
   ],
   "source": [
    "context_size = 4\n",
    "x = enc_sample[:context_size]\n",
    "y = enc_sample[1:context_size+1] \n",
    "\n",
    "print(f\"x: {x}\")\n",
    "print(f\"y: {y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "238e354f-a19c-41d9-962e-00eab8f64575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[290] ----------> 4920\n",
      "[290, 4920] ----------> 2241\n",
      "[290, 4920, 2241] ----------> 287\n",
      "[290, 4920, 2241, 287] ----------> 257\n"
     ]
    }
   ],
   "source": [
    "# Processing the inputs along with the targets, which are the inputs shifted by one position. We can then\n",
    "# create the next-word prediction tasks as follows:\n",
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "\n",
    "    print(context, \"---------->\", desired)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a243072-1fa3-4279-9cbe-39b966e728df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " and ---------->  established\n",
      " and established ---------->  himself\n",
      " and established himself ---------->  in\n",
      " and established himself in ---------->  a\n"
     ]
    }
   ],
   "source": [
    "# For illustration purposes, let's repeat the previous code but convert the token IDs into text\n",
    "for i in range(1, context_size+1):\n",
    "    context = enc_sample[:i]\n",
    "    desired = enc_sample[i]\n",
    "\n",
    "    print(tokenizer.decode(context), \"---------->\", tokenizer.decode([desired]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3140291c-483d-4b44-8467-d2cf13561983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step1: Tokenize the entire text\n",
    "# Step2: Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "# Step3: Return the total number of rows in the dataset\n",
    "# Step4: Return a single row from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e41dab0-2d49-45ac-9b20-fdf4023fe38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, txt, tokenizer, max_length, stride):\n",
    "        self.input_ids = []\n",
    "        self.target_ids = []\n",
    "\n",
    "        # Tokenize the entire text\n",
    "        token_ids = tokenizer.encode(txt, allowed_special={'<|endoftext|>'})\n",
    "        \n",
    "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
    "        for i in range(0, len(token_ids) - max_length, stride):\n",
    "            input_chunk = token_ids[i: i+max_length]\n",
    "            target_chunk = token_ids[i+1: i+max_length+1]\n",
    "            self.input_ids.append(torch.tensor(input_chunk))\n",
    "            self.target_ids.append(torch.tensor(target_chunk))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_ids[idx], self.target_ids[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ccb12eb-ac4d-4032-9486-0e538b0efbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step1: Initialize the tokenizer\n",
    "# Step2: Create dataset\n",
    "# Step3: drop_last = True drops the last batch if it is shorter than the specified batch_size to prevent loss spikes during training\n",
    "# Step4: The number of CPU processes to use for preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "384fb6c4-23df-4749-a189-b3cbb5f68e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
    "    \n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a0c745b3-ec32-4a44-b5e6-75550caa8814",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file in read mode\n",
    "with open('verdict.txt', 'r', encoding='utf-8') as file:\n",
    "    # Read the contents of the file\n",
    "    raw_text = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba474a6d-5a24-48ba-b77c-57356010867c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's test the dataloader with a batch size of 1 for an LLM with a context size of 4\n",
    "\n",
    "# Convert dataloader into a python iterator to fetch the next entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "078e000d-72c2-4b6c-9e65-1de732060adb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu124\n",
      "[tensor([[  40,  367, 2885, 1464]]), tensor([[ 367, 2885, 1464, 1807]])]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "\n",
    "dataloader = create_dataloader_v1(raw_text, batch_size=1, max_length=4, stride=1, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "first_batch = next(data_iter)\n",
    "print(first_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "13a8f5e5-17a9-44d3-9fd0-f278b12e9ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[tensor([[2885, 1464, 1807, 3619]]), tensor([[1464, 1807, 3619,  402]])]\n"
     ]
    }
   ],
   "source": [
    "second_batch = next(data_iter)\n",
    "print(second_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c1880ac0-4792-48cc-9852-0abf2aba4ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[   40,   367,  2885,  1464],\n",
      "        [ 1807,  3619,   402,   271],\n",
      "        [10899,  2138,   257,  7026],\n",
      "        [15632,   438,  2016,   257],\n",
      "        [  922,  5891,  1576,   438],\n",
      "        [  568,   340,   373,   645],\n",
      "        [ 1049,  5975,   284,   502],\n",
      "        [  284,  3285,   326,    11]])\n",
      "\n",
      "Targets:\n",
      " tensor([[  367,  2885,  1464,  1807],\n",
      "        [ 3619,   402,   271, 10899],\n",
      "        [ 2138,   257,  7026, 15632],\n",
      "        [  438,  2016,   257,   922],\n",
      "        [ 5891,  1576,   438,   568],\n",
      "        [  340,   373,   645,  1049],\n",
      "        [ 5975,   284,   502,   284],\n",
      "        [ 3285,   326,    11,   287]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d2d36f-f47c-4ce4-a7ec-4c1a0af8e353",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40b3f5c-7f82-49ea-b1fd-9231a98df09a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
